import os
import h5py
import pickle
import subprocess
import pandas as pd
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import re
import lmdb
from tqdm import tqdm
from Bio import SeqIO
from loguru import logger
from torch.utils.data import Dataset, DataLoader
import json

# [ìœ í‹¸ë¦¬í‹°]
def clean_id(full_id):
    if '|' in full_id: return full_id.split('|')[1]
    return full_id

def is_valid_go_term(term):
    return bool(re.match(r"^GO:\d{7}$", str(term).strip()))

class DiamondESM2Processor:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = config['output_dir']
        self.model_path = os.path.join(self.output_dir, "models", "head_model.pt")

    def load_go_mapping(self, tsv_path):
        df = pd.read_csv(tsv_path, sep='\t')
        # âœ… 'protein_id' â†’ 'EntryID', 'go_id' â†’ 'term'
        return df.groupby('EntryID')['term'].apply(lambda x: list(set(x))).to_dict()

    def generate_label_list(self, tsv_path, output_path):
        df = pd.read_csv(tsv_path, sep='\t')
        # âœ… 'go_id' â†’ 'term'
        all_labels = sorted(df['term'].unique().tolist())
        with open(output_path, 'wb') as f: 
            pickle.dump(all_labels, f)
        return len(all_labels)

    def clean_id(self, header):
        """
        [ìˆ˜ì •ë¨] header ë¬¸ìì—´ì—ì„œ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        selfëŠ” í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìì²´ì´ë¯€ë¡œ, 'in' ì—°ì‚°ì€ headerì— ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # headerê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
        header = str(header) 
        
        if "|" in header:
            # sp|ID|NAME í˜•ì‹ì¸ ê²½ìš° ë‘ ë²ˆì§¸ ìš”ì†Œ ë°˜í™˜
            return header.split("|")[1]
        
        # ê³µë°±ì´ ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
        return header.split()[0]

    def build_diamond_lmdb(self, fasta_in, go_mapping, lmdb_path, db_out, pkl_path=None, npz_path=None):
        # 1. ì¡°ìƒ ì •ë³´ ë¡œë“œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        go_map_data = go_mapping
        ancestor_map = {}
        if pkl_path and npz_path and os.path.exists(pkl_path):
            with open(pkl_path, 'rb') as f: 
                go_ids = pickle.load(f)
            matrix = sp.load_npz(npz_path)
            r, c = matrix.nonzero()
            for row, col in zip(r, c):
                if row != col:
                    child, parent = go_ids[row], go_ids[col]
                    if child not in ancestor_map: ancestor_map[child] = set()
                    ancestor_map[child].add(parent)

        # 2. LMDB êµ¬ì¶•
        env = lmdb.open(lmdb_path, map_size=20 * 1024**3)
        with env.begin(write=True) as txn:
            for record in tqdm(SeqIO.parse(fasta_in, "fasta"), desc="ğŸ“¦ LMDB(JSON) êµ¬ì¶•"):
                # [ì¤‘ìš”] acc_idê°€ 'A0A0C5B5G6'ê°€ ë©ë‹ˆë‹¤.
                acc_id = self.clean_id(record.id) 
                
                desc = record.description
                org_name = "Unknown"
                org_id = "0"
                
                if "OS=" in desc:
                    os_part = desc.split("OS=")[1]
                    # "Homo sapiens" ì¶”ì¶œ (strain ì •ë³´ ì œì™¸)
                    full_org_name = os_part.split(" OX=")[0].strip()
                    org_name = full_org_name.split(" (")[0].strip()
                    
                    # "9606" ì¶”ì¶œ
                    if "OX=" in os_part:
                        org_id = os_part.split("OX=")[1].split()[0].strip()

                # GO Term ë§¤ì¹­ ë° í™•ì¥
                terms = set(go_map_data.get(acc_id, []))
                expanded = terms.copy()
                for t in terms: 
                    expanded.update(ancestor_map.get(t, []))
                
                # [ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜] JSON ë°ì´í„° êµ¬ì¡°
                data_dict = {
                    "tax_id": acc_id,        # ì§ˆë¬¸ì—ì„œ ë§ì”€í•˜ì‹  ëŒ€ë¡œ ë‹¨ë°±ì§ˆ ID(A0A0C5B5G6)ë¥¼ ë„£ìŒ
                    "org_id": org_id,        # 9606 (Taxonomy ID)
                    "org_name": org_name,    # Homo sapiens (ì¢… ì´ë¦„)
                    "go_terms": sorted(list(expanded))
                }
                
                # JSON ì§ë ¬í™”
                json_value = json.dumps(data_dict, ensure_ascii=False)
                
                # [í•µì‹¬] LMDBì˜ Keyë¥¼ 'A0A0C5B5G6'ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                txn.put(acc_id.encode('utf-8'), json_value.encode('utf-8'))
                
        env.close()
        
        # 3. DIAMOND DB ìƒì„± (configì˜ ìŠ¤ë ˆë“œ ê°’ ì ìš©)
        num_threads = self.config.get('threads', 4)
        subprocess.run(["diamond", "makedb", "--in", fasta_in, "-d", db_out, "-p", str(num_threads)], check=True)

    def run_diamond_search(self, query_fasta, db_path, result_tsv):
        logger.info(f"ğŸ’ Diamond Search (Threads: {self.config['threads']})")
        cmd = ["diamond", "blastp", "-q", query_fasta, "-d", db_path, "-o", result_tsv, 
               "-p", str(self.config['threads']), "--max-target-seqs", "1", "--outfmt", "6"]
        subprocess.run(cmd, check=True)

    def final_ensemble(self, result_hits, lmdb_path, esm_preds=None, label_list_path=None):
        """
        ì•™ìƒë¸”ìš© ê³ ì •ë°€ë„ Diamond ì»´í¬ë„ŒíŠ¸
        """
        import json
        import pandas as pd
        from tqdm import tqdm
        import lmdb
        
        columns = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 
                'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore']
        
        try:
            dmnd_df = pd.read_csv(result_hits, sep='\t', names=columns)
            initial_count = len(dmnd_df)
            
            # âœ… ì•™ìƒë¸”ìš© ê³ ì •ë°€ë„ í•„í„°
            pident_threshold = 50
            dmnd_df = dmnd_df[
                (dmnd_df['pident'] >= pident_threshold) &  # ì¡°ì • ê°€ëŠ¥ (40, 50, 60)
                (dmnd_df['evalue'] <= 1e-10) &             # ê·¹ë„ë¡œ ì—„ê²©
                (dmnd_df['bitscore'] >= 100) &             # ë†’ì€ í’ˆì§ˆ
                (dmnd_df['length'] >= 80)                  # ê¸´ alignment
            ]
            
            logger.info(f"High-precision filtering (pidentâ‰¥{pident_threshold}): "
                    f"{initial_count} -> {len(dmnd_df)} hits ({len(dmnd_df)/initial_count*100:.1f}%)")
            
            dmnd_dict = {k: v for k, v in dmnd_df.groupby('qseqid')}
            
        except Exception as e:
            logger.warning(f"âš ï¸ Diamond ê²°ê³¼ ë¡œë“œ/í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return pd.DataFrame(columns=['Protein Id', 'GO Term Id', 'Prediction'])
        
        # LMDB ì¡°íšŒ
        env = lmdb.open(str(lmdb_path), readonly=True, lock=False)
        final_subs = []
        
        with env.begin() as txn:
            for qid, hits in tqdm(dmnd_dict.items(), desc="High-Precision Diamond"):
                combined_scores = {}
                
                for _, row in hits.iterrows():
                    sseqid = self.clean_id(row['sseqid'])
                    val = txn.get(sseqid.encode('utf-8'))
                    
                    if val:
                        data = json.loads(val.decode('utf-8'))
                        go_list = data.get('go_terms', [])
                        
                        if go_list:
                            # âœ… ë†’ì€ pidentëŠ” ë†’ì€ ì‹ ë¢°ë„
                            confidence = row['pident'] / 100.0
                            
                            for go_id in go_list:
                                combined_scores[go_id] = max(
                                    combined_scores.get(go_id, 0), 
                                    confidence
                                )
                
                # âœ… ë†’ì€ threshold (ê³ ì •ë°€ë„)
                for go_id, s in combined_scores.items():
                    if s >= 0.40:  # 40% ì´ìƒ (ë§¤ìš° ì—„ê²©)
                        final_subs.append([qid, go_id, round(s, 3)])
        
        env.close()
        logger.info(f"High-precision predictions: {len(final_subs)}")
        return pd.DataFrame(final_subs, columns=['Protein Id', 'GO Term Id', 'Prediction'])

    def create_cafa_submission(self, df, team_name, model_num):
        out_file = os.path.join(self.output_dir, f"submission_{model_num}.tsv")
        with open(out_file, 'w') as f:
            f.write(f"AUTHOR {team_name}\nMODEL {model_num}\nKEYWORDS Diamond-LMDB\n")
            df.to_csv(f, sep='\t', index=False, header=False)
        return out_file
    
    def evaluate_diamond_only(self, result_tsv, lmdb_path, label_list_path):
        """Diamond BLAST ê²°ê³¼ë§Œìœ¼ë¡œ í‰ê°€ - JSON ëŒ€ì‘ ë²„ì „"""
        logger.info("ğŸ“Š [Ablation] Diamond-only evaluation (JSON parsing)...")
        
        try:
            # ê²°ê³¼ ë¡œë“œ
            dmnd_df = pd.read_csv(result_tsv, sep='\t', names=['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore'])
            dmnd_dict = {k: v for k, v in dmnd_df.groupby('qseqid')}
            
            env = lmdb.open(str(lmdb_path), readonly=True, lock=False)
            diamond_subs = []
            
            with env.begin() as txn:
                for qid, hits in tqdm(dmnd_dict.items(), desc="ğŸ” Analyzing Hits"):
                    comb = {}
                    for _, row in hits.iterrows():
                        # âœ… ê²€ìƒ‰ ê²°ê³¼ IDë„ clean_idë¡œ ì •ì œí•´ì„œ ì¡°íšŒ
                        sseqid = self.clean_id(row['sseqid'])
                        val = txn.get(sseqid.encode('utf-8'))
                        
                        if val:
                            # âœ… í•µì‹¬ ìˆ˜ì •: JSON ë¡œë“œ
                            data = json.loads(val.decode('utf-8'))
                            go_list = data.get('go_terms', [])
                            
                            score = row['pident'] / 100.0
                            for go_id in go_list:
                                if is_valid_go_term(go_id):
                                    comb[go_id] = max(comb.get(go_id, 0), score)
                    
                    for go_id, f_score in comb.items():
                        diamond_subs.append([qid, go_id, round(f_score, 3)])
            
            env.close()
            diamond_df = pd.DataFrame(diamond_subs, columns=['Protein Id', 'GO Term Id', 'Prediction'])
            output_file = os.path.join(self.config['output_dir'], "diamond_only_submission.tsv")
            diamond_df.to_csv(output_file, sep='\t', index=False)
            
            logger.success(f"âœ… Diamond-only predictions: {len(diamond_subs)}")
            return diamond_df
            
        except Exception as e:
            logger.error(f"âŒ Diamond-only evaluation failed: {e}")
            raise