import os
import h5py
import pickle
import subprocess
import pandas as pd
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import re
import lmdb
from tqdm import tqdm
from Bio import SeqIO
from loguru import logger
from torch.utils.data import Dataset, DataLoader
import json
from collections import defaultdict

# [ìœ í‹¸ë¦¬í‹°]
def clean_id(full_id):
    if '|' in full_id: return full_id.split('|')[1]
    return full_id

def is_valid_go_term(term):
    return bool(re.match(r"^GO:\d{7}$", str(term).strip()))

class DiamondESM2Processor:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = config['output_dir']
        self.threads = config.get("threads", 14)        # default 14
        self.fs_score = config.get("fs_score", 0.99)    # default 0.99
        self.pident = config.get("pident", 50)          # default 50
        self.evalue = config.get("evalue", 1e-5)        # default 1e-5

    def load_go_mapping(self, tsv_path):
        """GO IDë³„ Namespace ì •ë³´ë¥¼ í•¨ê»˜ ì €ìž¥í•˜ë„ë¡ ìˆ˜ì •"""
        logger.info(f"ðŸ“‚ GO ë§¤í•‘ ë¡œë“œ ì¤‘: {tsv_path}")
        df = pd.read_csv(tsv_path, sep='\t')
        
        # Protein ID -> GO terms ë§¤í•‘
        mapping = df.groupby('EntryID')['term'].apply(lambda x: list(set(x))).to_dict()
        
        # âœ… í•µì‹¬: GO Term -> Namespace ë§¤í•‘ ì €ìž¥
        # ì»¬ëŸ¼ëª…ì´ 'namespace' ë˜ëŠ” 'aspect'ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
        if 'namespace' in df.columns:
            # ðŸ” ë¡œê¹… ì¶”ê°€ ì‹œìž‘
            unique_ns = df['namespace'].unique()
            logger.info(f"ðŸ” Unique namespaces found: {unique_ns}")
            logger.info(f"ðŸ“Š Total GO terms: {len(df['term'].unique())}")
            # ðŸ” ë¡œê¹… ì¶”ê°€ ë
            self.go_info_dict = pd.Series(df.namespace.values, index=df.term).to_dict()
        elif 'aspect' in df.columns:
            # ðŸ” ë¡œê¹… ì¶”ê°€ ì‹œìž‘
            unique_ns = df['aspect'].unique()
            logger.info(f"ðŸ” Unique aspects found: {unique_ns}")
            logger.info(f"ðŸ“Š Total GO terms: {len(df['term'].unique())}")
            # ðŸ” ë¡œê¹… ì¶”ê°€ ë
            self.go_info_dict = pd.Series(df.aspect.values, index=df.term).to_dict()
        else:
            raise ValueError("âŒ 'namespace' ë˜ëŠ” 'aspect' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        return mapping

    def generate_label_list(self, tsv_path, output_path):
        df = pd.read_csv(tsv_path, sep='\t')
        # âœ… 'go_id' â†’ 'term'
        all_labels = sorted(df['term'].unique().tolist())
        with open(output_path, 'wb') as f: 
            pickle.dump(all_labels, f)
        return len(all_labels)

    def clean_id(self, header):
        """
        [ìˆ˜ì •ë¨] header ë¬¸ìžì—´ì—ì„œ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        selfëŠ” í´ëž˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìžì²´ì´ë¯€ë¡œ, 'in' ì—°ì‚°ì€ headerì— ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # headerê°€ ë¬¸ìžì—´ì¸ì§€ í™•ì¸ (ì•ˆì „ìž¥ì¹˜)
        header = str(header) 
        
        if "|" in header:
            # sp|ID|NAME í˜•ì‹ì¸ ê²½ìš° ë‘ ë²ˆì§¸ ìš”ì†Œ ë°˜í™˜
            return header.split("|")[1]
        
        # ê³µë°±ì´ ìžˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
        return header.split()[0]

    def build_diamond_lmdb(self, fasta_in, go_mapping, lmdb_path, db_out, pkl_path=None, npz_path=None):
        # 1. ì¡°ìƒ ì •ë³´ ë¡œë“œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        go_map_data = go_mapping
        ancestor_map = {}
        if pkl_path and npz_path and os.path.exists(pkl_path):
            with open(pkl_path, 'rb') as f: 
                go_ids = pickle.load(f)
            matrix = sp.load_npz(npz_path)
            r, c = matrix.nonzero()
            for row, col in zip(r, c):
                if row != col:
                    child, parent = go_ids[row], go_ids[col]
                    if child not in ancestor_map: ancestor_map[child] = set()
                    ancestor_map[child].add(parent)

        # 2. LMDB êµ¬ì¶•
        env = lmdb.open(lmdb_path, map_size=20 * 1024**3)
        with env.begin(write=True) as txn:
            for record in tqdm(SeqIO.parse(fasta_in, "fasta"), desc="ðŸ“¦ LMDB(JSON) êµ¬ì¶•"):
                # [ì¤‘ìš”] acc_idê°€ 'A0A0C5B5G6'ê°€ ë©ë‹ˆë‹¤.
                acc_id = self.clean_id(record.id) 
                
                desc = record.description
                org_name = "Unknown"
                org_id = "0"
                
                if "OS=" in desc:
                    os_part = desc.split("OS=")[1]
                    # "Homo sapiens" ì¶”ì¶œ (strain ì •ë³´ ì œì™¸)
                    full_org_name = os_part.split(" OX=")[0].strip()
                    org_name = full_org_name.split(" (")[0].strip()
                    
                    # "9606" ì¶”ì¶œ
                    if "OX=" in os_part:
                        org_id = os_part.split("OX=")[1].split()[0].strip()

                # GO Term ë§¤ì¹­ ë° í™•ìž¥
                terms = set(go_map_data.get(acc_id, []))

                ### ë¶€ëª¨ì „íŒŒ O
                expanded = terms.copy()
                for t in terms: 
                    expanded.update(ancestor_map.get(t, []))
                data_dict = {
                    "protein_id": acc_id,    # ì§ˆë¬¸ì—ì„œ ë§ì”€í•˜ì‹  ëŒ€ë¡œ ë‹¨ë°±ì§ˆ ID(A0A0C5B5G6)ë¥¼ ë„£ìŒ
                    "org_id": org_id,        # 9606 (Taxonomy ID)
                    "org_name": org_name,    # Homo sapiens (ì¢… ì´ë¦„)
                    "go_terms": sorted(list(expanded))
                }
                
                # JSON ì§ë ¬í™”
                json_value = json.dumps(data_dict, ensure_ascii=False)
                
                # [í•µì‹¬] LMDBì˜ Keyë¥¼ 'A0A0C5B5G6'ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.
                txn.put(acc_id.encode('utf-8'), json_value.encode('utf-8'))
                
        env.close()
        
        # 3. DIAMOND DB ìƒì„± (configì˜ ìŠ¤ë ˆë“œ ê°’ ì ìš©)
        num_threads = self.config.get('threads', 4)
        subprocess.run(["diamond", "makedb", "--in", fasta_in, "-d", db_out, "-p", str(num_threads)], check=True)

    def run_diamond_search(self, query_fasta, db_path, result_tsv):
        logger.info(f"ðŸ’Ž Diamond Search (Threads: {self.config['threads']})")
        cmd = ["diamond", "blastp", "-q", query_fasta, "-d", db_path, "-o", result_tsv, 
               "-p", str(self.config['threads']), "--max-target-seqs", "1", "--outfmt", "6"]
        subprocess.run(cmd, check=True)

    def final_ensemble(self, dmnd_hits, lmdb_path, interpro_path=None, submission_path=None):
        """ âœ… Diamond + LMDB ë‹¨ë… ëª¨ë“œ """
        
        logger.info("ðŸš€ Diamond-only ëª¨ë“œ ì‹¤í–‰")
        
        combined_dict = defaultdict(lambda: defaultdict(float))
        
        try:
            dmnd_df = pd.read_csv(dmnd_hits, sep='\t', names=['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore'])
            dmnd_df = dmnd_df[(dmnd_df['pident'] >= self.pident) & (dmnd_df['evalue'] <= self.evalue)]

            env = lmdb.open(str(lmdb_path), readonly=True, lock=False)
            with env.begin() as txn:
                for qid, group in dmnd_df.groupby('qseqid'):
                    term_ev = defaultdict(list)
                    for _, row in group.iterrows():
                        s_id = self.clean_id(row['sseqid'])
                        val = txn.get(s_id.encode('utf-8'))
                        if val:
                            data = json.loads(val.decode('utf-8'))
                            go_list = data.get('go_terms', [])
                            conf = float(row['pident'] / 100.0)
                            for t in go_list:
                                term_ev[t].append(conf)
                    for t, evs in term_ev.items():
                        combined_dict[qid][t] = float(1.0 - np.prod([1.0 - e for e in evs]))
            env.close()
            
        except Exception as e:
            logger.error(f"âŒ Diamond ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return pd.DataFrame(columns=['Protein Id', 'GO Term Id', 'Prediction'])

        final_results = []
        for qid, terms in combined_dict.items():
            for tid, score in terms.items():
                final_results.append([qid, tid, round(score, 3)])
        
        output = pd.DataFrame(final_results, columns=['Protein Id', 'GO Term Id', 'Prediction'])
        logger.success(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(output)}ê±´")
        return output
