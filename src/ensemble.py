import os
import h5py
import pickle
import subprocess
import pandas as pd
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import re
import lmdb
from tqdm import tqdm
from Bio import SeqIO
from loguru import logger
from torch.utils.data import Dataset, DataLoader
import json
from collections import defaultdict

# [ìœ í‹¸ë¦¬í‹°]
def clean_id(full_id):
    if '|' in full_id: return full_id.split('|')[1]
    return full_id

def is_valid_go_term(term):
    return bool(re.match(r"^GO:\d{7}$", str(term).strip()))

class DiamondESM2Processor:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = config['output_dir']
        self.threads = config.get("threads", 14)        # default 14
        self.fs_score = config.get("fs_score", 0.99)    # default 0.99
        self.pident = config.get("pident", 50)          # default 50
        self.evalue = config.get("evalue", 1e-5)        # default 1e-5

    def load_go_mapping(self, tsv_path):
        """GO IDë³„ Namespace ì •ë³´ë¥¼ í•¨ê»˜ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •"""
        logger.info(f"ğŸ“‚ GO ë§¤í•‘ ë¡œë“œ ì¤‘: {tsv_path}")
        df = pd.read_csv(tsv_path, sep='\t')
        
        # Protein ID -> GO terms ë§¤í•‘
        mapping = df.groupby('EntryID')['term'].apply(lambda x: list(set(x))).to_dict()
        
        # âœ… í•µì‹¬: GO Term -> Namespace ë§¤í•‘ ì €ì¥
        # ì»¬ëŸ¼ëª…ì´ 'namespace' ë˜ëŠ” 'aspect'ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
        if 'namespace' in df.columns:
            # ğŸ” ë¡œê¹… ì¶”ê°€ ì‹œì‘
            unique_ns = df['namespace'].unique()
            logger.info(f"ğŸ” Unique namespaces found: {unique_ns}")
            logger.info(f"ğŸ“Š Total GO terms: {len(df['term'].unique())}")
            # ğŸ” ë¡œê¹… ì¶”ê°€ ë
            self.go_info_dict = pd.Series(df.namespace.values, index=df.term).to_dict()
        elif 'aspect' in df.columns:
            # ğŸ” ë¡œê¹… ì¶”ê°€ ì‹œì‘
            unique_ns = df['aspect'].unique()
            logger.info(f"ğŸ” Unique aspects found: {unique_ns}")
            logger.info(f"ğŸ“Š Total GO terms: {len(df['term'].unique())}")
            # ğŸ” ë¡œê¹… ì¶”ê°€ ë
            self.go_info_dict = pd.Series(df.aspect.values, index=df.term).to_dict()
        else:
            raise ValueError("âŒ 'namespace' ë˜ëŠ” 'aspect' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        return mapping

    def generate_label_list(self, tsv_path, output_path):
        df = pd.read_csv(tsv_path, sep='\t')
        # âœ… 'go_id' â†’ 'term'
        all_labels = sorted(df['term'].unique().tolist())
        with open(output_path, 'wb') as f: 
            pickle.dump(all_labels, f)
        return len(all_labels)

    def clean_id(self, header):
        """
        [ìˆ˜ì •ë¨] header ë¬¸ìì—´ì—ì„œ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        selfëŠ” í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìì²´ì´ë¯€ë¡œ, 'in' ì—°ì‚°ì€ headerì— ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # headerê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
        header = str(header) 
        
        if "|" in header:
            # sp|ID|NAME í˜•ì‹ì¸ ê²½ìš° ë‘ ë²ˆì§¸ ìš”ì†Œ ë°˜í™˜
            return header.split("|")[1]
        
        # ê³µë°±ì´ ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
        return header.split()[0]

    def build_diamond_lmdb(self, fasta_in, go_mapping, lmdb_path, db_out, pkl_path=None, npz_path=None):
        # 1. ì¡°ìƒ ì •ë³´ ë¡œë“œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        go_map_data = go_mapping
        ancestor_map = {}
        if pkl_path and npz_path and os.path.exists(pkl_path):
            with open(pkl_path, 'rb') as f: 
                go_ids = pickle.load(f)
            matrix = sp.load_npz(npz_path)
            r, c = matrix.nonzero()
            for row, col in zip(r, c):
                if row != col:
                    child, parent = go_ids[row], go_ids[col]
                    if child not in ancestor_map: ancestor_map[child] = set()
                    ancestor_map[child].add(parent)

        # 2. LMDB êµ¬ì¶•
        env = lmdb.open(lmdb_path, map_size=20 * 1024**3)
        with env.begin(write=True) as txn:
            for record in tqdm(SeqIO.parse(fasta_in, "fasta"), desc="ğŸ“¦ LMDB(JSON) êµ¬ì¶•"):
                # [ì¤‘ìš”] acc_idê°€ 'A0A0C5B5G6'ê°€ ë©ë‹ˆë‹¤.
                acc_id = self.clean_id(record.id) 
                
                desc = record.description
                org_name = "Unknown"
                org_id = "0"
                
                if "OS=" in desc:
                    os_part = desc.split("OS=")[1]
                    # "Homo sapiens" ì¶”ì¶œ (strain ì •ë³´ ì œì™¸)
                    full_org_name = os_part.split(" OX=")[0].strip()
                    org_name = full_org_name.split(" (")[0].strip()
                    
                    # "9606" ì¶”ì¶œ
                    if "OX=" in os_part:
                        org_id = os_part.split("OX=")[1].split()[0].strip()

                # GO Term ë§¤ì¹­ ë° í™•ì¥
                terms = set(go_map_data.get(acc_id, []))

                ### ë¶€ëª¨ì „íŒŒ O
                expanded = terms.copy()
                for t in terms: 
                    expanded.update(ancestor_map.get(t, []))
                data_dict = {
                    "protein_id": acc_id,    # ì§ˆë¬¸ì—ì„œ ë§ì”€í•˜ì‹  ëŒ€ë¡œ ë‹¨ë°±ì§ˆ ID(A0A0C5B5G6)ë¥¼ ë„£ìŒ
                    "org_id": org_id,        # 9606 (Taxonomy ID)
                    "org_name": org_name,    # Homo sapiens (ì¢… ì´ë¦„)
                    "go_terms": sorted(list(expanded))
                }
                
                # JSON ì§ë ¬í™”
                json_value = json.dumps(data_dict, ensure_ascii=False)
                
                # [í•µì‹¬] LMDBì˜ Keyë¥¼ 'A0A0C5B5G6'ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                txn.put(acc_id.encode('utf-8'), json_value.encode('utf-8'))
                
        env.close()
        
        # 3. DIAMOND DB ìƒì„± (configì˜ ìŠ¤ë ˆë“œ ê°’ ì ìš©)
        num_threads = self.config.get('threads', 4)
        subprocess.run(["diamond", "makedb", "--in", fasta_in, "-d", db_out, "-p", str(num_threads)], check=True)

    def run_diamond_search(self, query_fasta, db_path, result_tsv):
        logger.info(f"ğŸ’ Diamond Search (Threads: {self.config['threads']})")
        cmd = ["diamond", "blastp", "-q", query_fasta, "-d", db_path, "-o", result_tsv, 
               "-p", str(self.config['threads']), "--max-target-seqs", "1", "--outfmt", "6"]
        subprocess.run(cmd, check=True)

    def final_ensemble(self, dmnd_hits, lmdb_path, interpro_path=None, submission_path=None):
        if not hasattr(self, 'go_info_dict') or self.go_info_dict is None:
            raise ValueError("âŒ go_info_dictê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        mf_terms = {term for term, ns in self.go_info_dict.items() 
                    if ns in ['MFO', 'molecular_function', 'F']}
        logger.info(f"âœ… MF Terms: {len(mf_terms)}")
        # 1. Diamond ì ìˆ˜ ì‚°ì¶œ (MF ì„±ëŠ¥ ë³´ì¡´ì˜ í•µì‹¬)
        combined_dict = defaultdict(lambda: defaultdict(float))
        try:
            dmnd_df = pd.read_csv(dmnd_hits, sep='\t', names=['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore'])
            dmnd_df = dmnd_df[(dmnd_df['pident'] >= self.pident) & (dmnd_df['evalue'] <= self.evalue)]

            env = lmdb.open(str(lmdb_path), readonly=True, lock=False)
            with env.begin() as txn:
                for qid, group in dmnd_df.groupby('qseqid'):
                    term_ev = defaultdict(list)
                    for _, row in group.iterrows():
                        val = txn.get(self.clean_id(row['sseqid']).encode('utf-8'))
                        if val:
                            go_list = json.loads(val.decode('utf-8')).get('go_terms', [])
                            conf = float(row['pident'] / 100.0)
                            for t in go_list: term_ev[t].append(conf)
                    for t, evs in term_ev.items():
                        combined_dict[qid][t] = float(1.0 - np.prod([1.0 - e for e in evs]))
            env.close()
        except Exception as e:
            logger.warning(f"Diamond ì²˜ë¦¬ ì¤‘ ì•Œë¦¼: {e}")

        df_diamond = pd.DataFrame([[q, t, s] for q, t_dict in combined_dict.items() for t, s in t_dict.items()], columns=['id', 'term', 'score_dmnd'])

        # âœ… MF Term ë¶„ë¦¬
        mf_terms = {term for term, ns in self.go_info_dict.items() 
                    if ns in ['MFO', 'molecular_function', 'F']}

        # âœ… í—¬í¼ í•¨ìˆ˜
        def load_and_filter(path, col_name):
            if path and os.path.exists(str(path)):
                df = pd.read_csv(path, sep='\t', header=None, names=['id', 'term', col_name])
                if len(df) > 0:
                    logger.info(f"ğŸ“Š {col_name} score range: {df[col_name].min():.3f} ~ {df[col_name].max():.3f}")
                df = df[~df['term'].isin(mf_terms)]
                return df[df[col_name] >= 0.6]
            return pd.DataFrame(columns=['id', 'term', col_name])

        # âœ… ë°ì´í„° ë¡œë“œ
        df_base = load_and_filter(interpro_path, 'score_base')
        df_patch = load_and_filter(submission_path, 'score_patch')

        # âœ… ë³‘í•© (None ì²´í¬)
        if df_base.empty:
            merged = df_patch.copy() if not df_patch.empty else pd.DataFrame(columns=['id', 'term'])
        else:
            merged = pd.merge(df_base, df_patch, on=['id', 'term'], how='outer')
        merged = pd.merge(merged, df_diamond, on=['id', 'term'], how='outer')

        # âœ… MAX ê¸°ë°˜ ì•™ìƒë¸”
        score_cols = ['score_dmnd', 'score_patch', 'score_base']
        merged['final'] = merged[score_cols].max(axis=1, skipna=True)

        output = merged[['id', 'term', 'final']].dropna(subset=['final']).copy()
        output['final'] = output['final'].round(3)

        logger.success(f"âœ… ì•™ìƒë¸” ì™„ë£Œ: MF(Diamond) {len(output[output['term'].isin(mf_terms)])} + BP/CC {len(output[~output['term'].isin(mf_terms)])}")
        return output
    