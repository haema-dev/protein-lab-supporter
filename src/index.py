import os
import argparse
import subprocess
import torch
import pandas as pd
import pickle
from loguru import logger
import traceback
import glob
import math

# [1] ëª¨ë“ˆ ë¡œë“œ (ê¸°ì¡´ ensemble.pyì— DiamondESM2Processorê°€ ìˆë‹¤ê³  ê°€ì •)
from ensemble import DiamondESM2Processor


def convert_size(size_bytes):
    """
    íŒŒì¼ ìš©ëŸ‰ ì²´í¬
    """
    if size_bytes == 0:
        return "0B"
    size_name = ("B", "KB", "MB", "GB", "TB", "PB")
    i = int(math.floor(math.log(size_bytes, 1024)))
    p = math.pow(1024, i)
    s = round(size_bytes / p, 2)
    return f"{s} {size_name[i]}"

def find_file(directory, pattern, required=False):
    """
    ê°œì„ ëœ íŒŒì¼ ì°¾ê¸° í•¨ìˆ˜: 
    1. í™•ì¥ìë§Œ ë„£ì–´ë„ ìë™ìœ¼ë¡œ '*'ë¥¼ ë¶™ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    2. í‚¤ì›Œë“œ ë§¤ì¹­ì„ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    
    if not os.path.isdir(directory):
        if required:
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {os.path.abspath(directory)}")
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ ì—†ìŒ: {directory}")
        return None
    
    # íŒ¨í„´ ë³´ì •: ".fasta" -> "*.fasta"
    search_pattern = pattern
    if not search_pattern.startswith("*"):
        search_pattern = f"*{search_pattern}"
    
    candidates = list(Path(directory).glob(search_pattern))
        
    found_path = str(candidates[0])
    logger.info(f"ğŸ” íŒŒì¼ ë°œê²¬: {os.path.basename(found_path)}")
    return found_path

def check_model_sizes(directory, extension="*.pt"):
    """
    íŠ¹ì • í´ë” ë‚´ ëª¨ë¸ íŒŒì¼ë“¤ë§Œ ì²´í¬
    """
    files = glob.glob(os.path.join(directory, extension))
    
    logger.info(f"Checking {len(files)} files in {directory}...")
    
    total_size = 0
    
    for file in files:
        size = os.path.getsize(file)
        total_size += size
        logger.info(f"File: {os.path.basename(file)} | Size: {convert_size(size)}")
        
    logger.success(f"Total Size: {convert_size(total_size)}")

def main():
    parser = argparse.ArgumentParser(description="DiamondDB + LMDB")
    
    # ================== 1. config ì„¸íŒ… ==================
    # Azure ML ê²½ë¡œ ì„¤ì •
    parser.add_argument('--data_path', type=str, required=True, help='dataset í´ë” ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, default='./outputs', help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--threads', type=int, default=14)
    parser.add_argument('--fs_score', type=float, default=0.99)
    parser.add_argument('--pident', type=int, default=50)
    parser.add_argument('--evalue', type=float, default=1e-5)
    # === í•„ìš”í•˜ë©´ ì£¼ì„í•´ì œ í›„ ì‚¬ìš©í•˜ê¸°
    # parser.add_argument('--train_batch_size', type=int, default=1024, help='Head í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸° (H5 ê¸°ë°˜ì´ë¼ í¬ê²Œ ê°€ëŠ¥)')
    # parser.add_argument('--predict_batch_size', type=int, default=2048, help='ì¶”ë¡  ì‹œ ë°°ì¹˜ í¬ê¸°')

    args = parser.parse_args()
    
    # ================== 1. ê²½ë¡œ ë° í™˜ê²½ ì„¤ì • ==================
    # Root DIR ì„¤ì •
    DATASET_DIR = args.data_path
    OUTPUT_DIR = args.output_dir
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    logger.info(f"ğŸ“‚ Input/Output ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ:")
    logger.info(f"   - Input Dataset Root: {os.path.abspath(DATASET_DIR)}")
    logger.info(f"   - Output Root: {os.path.abspath(OUTPUT_DIR)}")

    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • ë° ìƒì„±
    # Azure MLì—ì„œëŠ” ./outputs í´ë” ì•ˆì— íŒŒì¼ì„ ë‘ë©´ ìë™ìœ¼ë¡œ ì•„í‹°íŒ©íŠ¸ë¡œ ìˆ˜ì§‘
    MODEL_DIR = os.path.join(OUTPUT_DIR, "models")
    os.makedirs(MODEL_DIR, exist_ok=True)

    # (ì˜µì…˜) ë¡œê·¸ë‚˜ ì„ì‹œ íŒŒì¼ì„ ìœ„í•œ í´ë”
    LOG_DIR = os.path.join(OUTPUT_DIR, "logs")
    os.makedirs(LOG_DIR, exist_ok=True)

    logger.info(f"ğŸ“‚ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ:")
    logger.info(f"   - Model Save Dir: {os.path.abspath(MODEL_DIR)}")
    logger.info(f"   - Log Save Dir: {os.path.abspath(LOG_DIR)}")

    # 1. ë””ë ‰í† ë¦¬ ì„¤ì •
    # [ Input dataset folder structure ]
    # â”€â”¬â”€ fasta (.fasta) :  large_learning_superset Data
    #  â”œâ”€ foldseek (.tsv)
    #  â”œâ”€ h5 (.h5) :  esm2-3b float16 ì„ë² ë”© ë°ì´í„°
    #  â”œâ”€ interpro (.tsv)
    #  â”œâ”€ ontology (.npz / .pkl) :  ë¶€ëª¨ì „íŒŒ íŒŒì¼
    #  â”œâ”€ tsv (.tsv) :  large_learning_superset Data
    #  â””â”€ validation :  ì±„ì  ë°ì´í„°
    FASTA_DIR = os.path.join(DATASET_DIR, "fasta")
    FOLDSEEK_DIR = os.path.join(DATASET_DIR, "foldseek")
    TSV_DIR = os.path.join(DATASET_DIR, "tsv")
    H5_DIR = os.path.join(DATASET_DIR, "h5")
    INTERPRO_DIR = os.path.join(DATASET_DIR, "interpro")
    ONTOLOGY_DIR = os.path.join(DATASET_DIR, "ontology")
    VALID_DIR = os.path.join(DATASET_DIR, "validation")

    # 2. ë™ì  íŒŒì¼ ì°¾ê¸° (ì´ë¦„ì´ ë‹¬ë¼ë„ í™•ì¥ìì™€ í‚¤ì›Œë“œë¡œ ìë™ ë§¤ì¹­)
    # Train ë°ì´í„°
    TRAIN_FASTA = find_file(FASTA_DIR, ".fasta")
    TRAIN_GO_TSV = find_file(TSV_DIR, ".tsv")
    TRAIN_H5    = find_file(H5_DIR, ".h5")

    # Ontology (NPZ, PKLì€ ë³´í†µ í•˜ë‚˜ë¿ì´ë¯€ë¡œ í™•ì¥ìë¡œ ê²€ìƒ‰)
    PARENTS_PKL = find_file(ONTOLOGY_DIR, ".pkl")
    PARENTS_NPZ = find_file(ONTOLOGY_DIR, ".npz")

    # Test / Validation ë°ì´í„°
    TEST_FASTA    = find_file(VALID_DIR, ".fasta") or find_file(VALID_DIR, ".fasta")
    TEST_H5       = find_file(VALID_DIR, ".h5") or find_file(VALID_DIR, ".h5")
    VALIDATION_GT = find_file(VALID_DIR, "*.tsv")

    # ê²½ë¡œ ê²€ì¦ ë¡œê·¸
    logger.info("ğŸ” ë™ì  ê²½ë¡œ íƒìƒ‰ ê²°ê³¼:")
    logger.info(f"  - Train FASTA: {TRAIN_FASTA}")
    logger.info(f"  - Train H5: {TRAIN_H5}")
    logger.info(f"  - Train ONTOLOGY: {ONTOLOGY_DIR}")
    logger.info(f"  - Test VALID: {VALID_DIR}")

    logger.info(f"ğŸš€ DiamondDB + LMDB íŒŒì´í”„ë¼ì¸ ì‹œì‘ (Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})")

    # ================== 2. í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ==================
    config = {
        'threads': args.threads,
        'output_dir': OUTPUT_DIR,
        'fs_score': args.fs_score,
        'pident': args.pident,
        'evalue': args.evalue,
        # 'train_batch_size': args.train_batch_size,
        # 'batch_size': args.predict_batch_size, 
        # 'embedding_dim': 2560 ### ìˆ˜ì •í•˜ë©´ ì•ˆ ë¨.
    }

    proc = DiamondESM2Processor(config)
    
    # ================== 3. [Phase 1] DiamondDB + LMDB ë§¤í•‘ì €ì¥ ==================
    try:
        # Step 1: GO Mapping ë¡œë“œ
        go_mapping = proc.load_go_mapping(TRAIN_GO_TSV)
        logger.success(f"âœ… {len(go_mapping)}ê°œ ë‹¨ë°±ì§ˆ-GO ë§¤í•‘ ë¡œë“œ!")
        
        # Step 2: Label List ìƒì„± (go_mappingì—ì„œ ì¶”ì¶œ)
        label_pkl = os.path.join(MODEL_DIR, "labels.pkl")
        proc.generate_label_list(TRAIN_GO_TSV, label_pkl)
        # íŒŒì¼ í™•ì¸ ë° ìš©ëŸ‰ ì²´í¬
        if os.path.exists(label_pkl):
            file_size = os.path.getsize(label_pkl)
            logger.success(f"âœ… labels.pkl ìƒì„± ì„±ê³µ!")
            logger.info(f"ğŸ“Š íŒŒì¼ ìœ„ì¹˜: {os.path.abspath(label_pkl)}")
            logger.info(f"ğŸ’¾ ëª¨ë¸(ë ˆì´ë¸”) íŒŒì¼ í¬ê¸°: {convert_size(file_size)}")
            # === í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ ===
            # [ê²€ì¦] ì‹¤ì œ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸
            # with open(label_pkl, 'rb') as f:
            #     loaded_labels = pickle.load(f)
            #     logger.info(f"ğŸ”¢ ì´ GO Term ê°œìˆ˜: {len(loaded_labels):,}ê°œ")
        else:
            logger.error(f"âŒ labels.pkl íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {label_pkl}")
        
        # Step 3: LMDB êµ¬ì¶• (go_mapping í™œìš©)
        lmdb_path = os.path.join(MODEL_DIR, "train_lmdb")
        dmnd_db = os.path.join(MODEL_DIR, "diamond_db.dmnd")
        proc.build_diamond_lmdb(
            TRAIN_FASTA,
            go_mapping,
            lmdb_path,
            dmnd_db,
            PARENTS_PKL,
            PARENTS_NPZ
        )
        logger.success("âœ… DiamondDB + LMDB ë§¤í•‘ì €ì¥ ì„±ê³µ!")

        # FS_DB = os.path.join(MODEL_DIR, "foldseek")
        # logger.success("âœ… DiamondDB + LMDB ë§¤í•‘ì €ì¥ ì„±ê³µ!")
        
    except Exception as e:
        logger.error(f"âŒ DiamondDB + LMDB ë§¤í•‘ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # ================== 4. [Phase 2] í•™ìŠµ (Training) ==================
    logger.info("ğŸ—ï¸ Phase 1: ë°ì´í„° ë¡œë“œ ë° í•™ìŠµ ì‹œì‘")
    
    ### í•™ìŠµ ë¡œì§ ìˆ˜í–‰
    
    logger.info("â© í•™ìŠµ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")

    # ================== 4. [Phase 3] ì¶”ë¡  (Inference) ==================
    logger.info("ğŸš€ Phase 2: ì¶”ë¡  ì‹œì‘")
    try:
        # 1. Diamond ê²€ìƒ‰
        dmnd_hits = os.path.join(OUTPUT_DIR, "dmnd_hits.tsv")
        proc.run_diamond_search(TEST_FASTA, dmnd_db, dmnd_hits)
        
        esm_preds = None
        
        ### 2. í•™ìŠµí•œ ëª¨ë¸ì¶”ê°€
        
        logger.info("í•™ìŠµí•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜í–‰: {}", esm_preds)
        
        # 3. ìµœì¢… ì•™ìƒë¸”
        INTERPRO_FILE = find_file(INTERPRO_DIR, ".tsv")
        FOLDSEEK_FILE = find_file(FOLDSEEK_DIR, ".tsv")

        final_df = proc.final_ensemble(
            dmnd_hits=dmnd_hits,
            lmdb_path=lmdb_path,
            interpro_path=INTERPRO_FILE,
            submission_path=FOLDSEEK_FILE
        )

        
        # 4. ê²°ê³¼ ì €ì¥
        final_save_path = os.path.join(OUTPUT_DIR, "final_results.tsv")
        final_df.to_csv(final_save_path, sep='\t', index=False)
        logger.success(f"âœ… ì¶”ë¡  ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨: {final_save_path}")

    except Exception as e:
        logger.error(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
        return

    # ================== 5. [Phase 4] Ablation Study ==================
    if VALIDATION_GT is not None and os.path.exists(VALIDATION_GT):
        logger.info("ğŸ”¬ Phase 3: Ablation Study & Evaluation")
        try:

            ### í•„ìš”í•˜ë©´ ì¶”ê°€í•˜ë©´ ë¨

            logger.success("âœ… í‰ê°€ ì™„ë£Œ!")
        except Exception as e:
            logger.warning(f"âš ï¸ í‰ê°€ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
    else:
        logger.warning("âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ìŠ¤í‚µ")

    logger.success("ğŸ CAFA6 í†µí•© íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ!")

if __name__ == "__main__":
    main()